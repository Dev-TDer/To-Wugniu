{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted 'data/translate_raw.txt' to 'data/translate.json'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def txt_to_json(input_file: str, output_file: str):\n",
    "    data_dict = {}\n",
    "\n",
    "    # Read the text file\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:  # Skip empty lines\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    # The first part is the key, the rest are joined to form the value\n",
    "                    key = parts[0]\n",
    "                    value = \" \".join(parts[1:])\n",
    "                    data_dict[key] = value\n",
    "\n",
    "    data_dict = dict(sorted(data_dict.items(), key=lambda x: x[0]))\n",
    "\n",
    "    # Save the dictionary as a JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data_dict, json_file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"Successfully converted '{input_file}' to '{output_file}'\")\n",
    "\n",
    "txt_to_json('data/translate_raw.txt', 'data/translate.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add words from wugniu_dict.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess wugniu_dict.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 272 entries.\n",
      "Filtered data saved to data/wugniu_dict_updated.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def remove_endings_but_keep_self(input_file: str, output_file: str):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Define endings to remove, and exceptions to keep\n",
    "    endings_to_remove = ('些', '的', '他', '你', '給', '與', '被', '誰', '呢', '掉')\n",
    "    exceptions_to_keep = set(endings_to_remove)\n",
    "\n",
    "    # Apply filter\n",
    "    filtered = {\n",
    "        key: value for key, value in data.items()\n",
    "        if not (key.endswith(endings_to_remove) and key not in exceptions_to_keep)\n",
    "    }\n",
    "\n",
    "    print(f\"Removed {len(data) - len(filtered)} entries.\")\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Filtered data saved to {output_file}\")\n",
    "\n",
    "\n",
    "remove_endings_but_keep_self('data/wugniu_dict_vocab.json', 'data/wugniu_dict_updated.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 140 entries starting with ['你', '妳', '被', '誰', '喝'].\n",
      "Filtered data saved to data/wugniu_dict_updated.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def remove_startswith_any_but_keep_self(input_file: str, output_file: str, prefixes: list):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    filtered = {\n",
    "        key: value for key, value in data.items()\n",
    "        if not any(key.startswith(p) and key != p for p in prefixes)\n",
    "    }\n",
    "\n",
    "    removed_count = len(data) - len(filtered)\n",
    "    print(f\"Removed {removed_count} entries starting with {prefixes}.\")\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Filtered data saved to {output_file}\")\n",
    "\n",
    "remove_startswith_any_but_keep_self(\n",
    "    'data/wugniu_dict_updated.json',\n",
    "    'data/wugniu_dict_updated.json',\n",
    "    prefixes=['你', '妳', '被', '誰', '喝']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add words from pre-processed dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated translate dictionary saved to data/translate.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def enrich_translate_with_matches(translate_file: str, wugniu_file: str, output_file: str):\n",
    "    # Load both JSON files\n",
    "    with open(translate_file, 'r', encoding='utf-8') as f:\n",
    "        translate_dict = json.load(f)\n",
    "\n",
    "    with open(wugniu_file, 'r', encoding='utf-8') as f:\n",
    "        wugniu_dict = json.load(f)\n",
    "\n",
    "    # Go through each single-character key in translate.json\n",
    "    for key, value in translate_dict.copy().items():\n",
    "        if len(key) == 1:\n",
    "            # Look for any word in wugniu_dict that contains this character\n",
    "            for word in wugniu_dict:\n",
    "                if key in word and word not in translate_dict:\n",
    "                    translate_dict[word] = word  # Add it as word: word\n",
    "\n",
    "    translate_dict = dict(sorted(translate_dict.items(), key=lambda x: x[0]))\n",
    "\n",
    "    # Save the updated translate dictionary\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(translate_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Updated translate dictionary saved to {output_file}\")\n",
    "\n",
    "enrich_translate_with_matches('data/translate.json', 'data/wugniu_dict_updated.json', 'data/translate.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped and sorted dictionary saved to data/translate.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict, OrderedDict\n",
    "\n",
    "def group_and_sort_translate(input_file: str, output_file: str):\n",
    "    # Load the dictionary\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        translate_dict = json.load(f)\n",
    "\n",
    "    # Create a grouping structure\n",
    "    groups = defaultdict(list)\n",
    "    singles = {}\n",
    "\n",
    "    for key, value in translate_dict.items():\n",
    "        if len(key) == 1:\n",
    "            singles[key] = value\n",
    "        else:\n",
    "            # Group under each single char that exists in the key and also in the single-char list\n",
    "            for char in key:\n",
    "                if char in translate_dict and len(char) == 1:\n",
    "                    groups[char].append((key, value))\n",
    "                    break\n",
    "            else:\n",
    "                # If no grouping character found, leave it ungrouped\n",
    "                groups[None].append((key, value))\n",
    "\n",
    "    # Now build the final sorted dictionary\n",
    "    final_dict = OrderedDict()\n",
    "\n",
    "    # Sort single chars\n",
    "    for char in sorted(singles):\n",
    "        final_dict[char] = singles[char]\n",
    "        # Sort multi-char entries under this char\n",
    "        for word, val in sorted(groups.get(char, []), key=lambda x: x[0]):\n",
    "            final_dict[word] = val\n",
    "\n",
    "    # Add ungrouped words at the end, sorted\n",
    "    for word, val in sorted(groups.get(None, []), key=lambda x: x[0]):\n",
    "        final_dict[word] = val\n",
    "\n",
    "    # Save the new JSON\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_dict, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Grouped and sorted dictionary saved to {output_file}\")\n",
    "\n",
    "group_and_sort_translate('data/translate.json', 'data/translate.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Simp. Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dictionary saved to data/translate.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from opencc import OpenCC\n",
    "from typing import Dict, Union\n",
    "\n",
    "def preserve_convert(text: str, exclude_char: str, cc: OpenCC) -> str:\n",
    "    \"\"\"Convert text to simplified Chinese, but preserve exclude_char.\"\"\"\n",
    "    return ''.join(char if char == exclude_char else cc.convert(char) for char in text)\n",
    "\n",
    "def add_simplified_forms(json_file_path: str, output_file_path: str) -> None:\n",
    "    cc = OpenCC('t2s')  # Traditional to Simplified Chinese\n",
    "    EXCLUDE_CHAR = \"箇\"\n",
    "\n",
    "    # Load the JSON file\n",
    "    try:\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "            words_dict: Dict[str, Union[str, list]] = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {json_file_path} was not found.\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Failed to decode JSON from {json_file_path}.\")\n",
    "        return\n",
    "\n",
    "    updated_dict: Dict[str, Union[str, list]] = {}\n",
    "\n",
    "    for traditional_word, translation in words_dict.items():\n",
    "        simplified_word = cc.convert(traditional_word)\n",
    "\n",
    "        # Apply special preserve logic to translation\n",
    "        if isinstance(translation, str):\n",
    "            simplified_translation = preserve_convert(translation, EXCLUDE_CHAR, cc)\n",
    "        elif isinstance(translation, list):\n",
    "            simplified_translation = [\n",
    "                preserve_convert(t, EXCLUDE_CHAR, cc) for t in translation\n",
    "            ]\n",
    "        else:\n",
    "            simplified_translation = translation  # Just in case of odd format\n",
    "\n",
    "        # Add original traditional pair\n",
    "        updated_dict.setdefault(traditional_word, translation)\n",
    "\n",
    "        # Add simplified version if key differs\n",
    "        if simplified_word != traditional_word:\n",
    "            if simplified_word in updated_dict:\n",
    "                if isinstance(updated_dict[simplified_word], list):\n",
    "                    if simplified_translation not in updated_dict[simplified_word]:\n",
    "                        updated_dict[simplified_word].append(simplified_translation)\n",
    "                else:\n",
    "                    if updated_dict[simplified_word] != simplified_translation:\n",
    "                        updated_dict[simplified_word] = [\n",
    "                            updated_dict[simplified_word],\n",
    "                            simplified_translation\n",
    "                        ]\n",
    "            else:\n",
    "                updated_dict[simplified_word] = simplified_translation\n",
    "\n",
    "    # Save updated dictionary\n",
    "    try:\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(updated_dict, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Updated dictionary saved to {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file: {e}\")\n",
    "\n",
    "# Run it\n",
    "add_simplified_forms('data/translate.json', 'data/translate.json')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
